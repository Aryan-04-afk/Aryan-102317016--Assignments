{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ir4fBdAr880L",
        "outputId": "8ee3d100-a7fc-4ac7-caba-acdf803200c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkAS-xTPA7VD"
      },
      "source": [
        " **Q1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports,\n",
        "technology, food, books, etc.).\n",
        "1.Convert text to lowercase and remove punctuaƟon.\n",
        "2.Tokenize the text into words and sentences.\n",
        "3.Remove stopwords (using NLTK's stopwords list).\n",
        "4.Display word frequency distribuƟon (excluding stopwords).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDIG1PcTTzD8",
        "outputId": "399bd13c-3208-4b52-c384-db59a41f620e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word Frequency (no stopwords):\n",
            "Counter({'new': 2, 'technology': 1, 'evolving': 1, 'rapid': 1, 'pace': 1, 'transforming': 1, 'every': 1, 'aspect': 1, 'lives': 1, 'smartphones': 1, 'aipowered': 1, 'assistants': 1, 'innovation': 1, 'everywhere': 1, 'love': 1, 'exploring': 1, 'gadgets': 1, 'reading': 1, 'latest': 1, 'tech': 1, 'trends': 1, 'understanding': 1, 'impact': 1, 'society': 1, 'amazing': 1, 'quickly': 1, 'software': 1, 'hardware': 1, 'changing': 1, 'advancement': 1, 'future': 1, 'becomes': 1, 'exciting': 1, 'unpredictable': 1})\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "\n",
        "text = \"\"\"Technology is evolving at a rapid pace, transforming every aspect of our lives. From smartphones to AI-powered assistants, innovation is everywhere. I love exploring new gadgets, reading about the latest tech trends, and understanding how they impact society. It's amazing how quickly software and hardware are changing. With each new advancement, the future becomes more exciting and unpredictable.\"\"\"\n",
        "\n",
        "#removing punctuation\n",
        "text_clean = re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "\n",
        "\n",
        "\n",
        "#tokenization\n",
        "words=word_tokenize(text_clean)\n",
        "\n",
        "sentences=sent_tokenize(text_clean)\n",
        "\n",
        "\n",
        "#remove stopwords\n",
        "stop_words=set(stopwords.words('english'))\n",
        "filtered_words=[word for word in words if word not in stop_words]\n",
        "\n",
        "\n",
        "#word frequency\n",
        "freq_dist=Counter(filtered_words)\n",
        "print(\"Word Frequency (no stopwords):\")\n",
        "print(freq_dist)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5fnxR0CBdz7"
      },
      "source": [
        "**Q2: Stemming and LemmaƟzaƟon\n",
        "1.Take the tokenized words from QuesƟon 1 (aŌer stopword removal).\n",
        "2.Apply stemming using NLTK's PorterStemmer and LancasterStemmer.\n",
        "3.Apply lemmaƟzaƟon using NLTK's WordNetLemmaƟzer.\n",
        "4.Compare and display results of both techniques.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iq7YuC8j9Hex",
        "outputId": "2f6e0b70-9b2f-4335-a566-ce51f2292d6d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stemming and Lemmatization Results:\n",
            "Word: technology\n",
            "Porter Stemmer: technolog\n",
            "Lancaster Stemmer: technolog\n",
            "Lemmatizer: technology\n",
            "\n",
            "Word: evolving\n",
            "Porter Stemmer: evolv\n",
            "Lancaster Stemmer: evolv\n",
            "Lemmatizer: evolving\n",
            "\n",
            "Word: rapid\n",
            "Porter Stemmer: rapid\n",
            "Lancaster Stemmer: rapid\n",
            "Lemmatizer: rapid\n",
            "\n",
            "Word: pace\n",
            "Porter Stemmer: pace\n",
            "Lancaster Stemmer: pac\n",
            "Lemmatizer: pace\n",
            "\n",
            "Word: transforming\n",
            "Porter Stemmer: transform\n",
            "Lancaster Stemmer: transform\n",
            "Lemmatizer: transforming\n",
            "\n",
            "Word: every\n",
            "Porter Stemmer: everi\n",
            "Lancaster Stemmer: every\n",
            "Lemmatizer: every\n",
            "\n",
            "Word: aspect\n",
            "Porter Stemmer: aspect\n",
            "Lancaster Stemmer: aspect\n",
            "Lemmatizer: aspect\n",
            "\n",
            "Word: lives\n",
            "Porter Stemmer: live\n",
            "Lancaster Stemmer: liv\n",
            "Lemmatizer: life\n",
            "\n",
            "Word: smartphones\n",
            "Porter Stemmer: smartphon\n",
            "Lancaster Stemmer: smartphon\n",
            "Lemmatizer: smartphones\n",
            "\n",
            "Word: aipowered\n",
            "Porter Stemmer: aipow\n",
            "Lancaster Stemmer: aipow\n",
            "Lemmatizer: aipowered\n",
            "\n",
            "Word: assistants\n",
            "Porter Stemmer: assist\n",
            "Lancaster Stemmer: assist\n",
            "Lemmatizer: assistant\n",
            "\n",
            "Word: innovation\n",
            "Porter Stemmer: innov\n",
            "Lancaster Stemmer: innov\n",
            "Lemmatizer: innovation\n",
            "\n",
            "Word: everywhere\n",
            "Porter Stemmer: everywher\n",
            "Lancaster Stemmer: everywh\n",
            "Lemmatizer: everywhere\n",
            "\n",
            "Word: love\n",
            "Porter Stemmer: love\n",
            "Lancaster Stemmer: lov\n",
            "Lemmatizer: love\n",
            "\n",
            "Word: exploring\n",
            "Porter Stemmer: explor\n",
            "Lancaster Stemmer: expl\n",
            "Lemmatizer: exploring\n",
            "\n",
            "Word: new\n",
            "Porter Stemmer: new\n",
            "Lancaster Stemmer: new\n",
            "Lemmatizer: new\n",
            "\n",
            "Word: gadgets\n",
            "Porter Stemmer: gadget\n",
            "Lancaster Stemmer: gadget\n",
            "Lemmatizer: gadget\n",
            "\n",
            "Word: reading\n",
            "Porter Stemmer: read\n",
            "Lancaster Stemmer: read\n",
            "Lemmatizer: reading\n",
            "\n",
            "Word: latest\n",
            "Porter Stemmer: latest\n",
            "Lancaster Stemmer: latest\n",
            "Lemmatizer: latest\n",
            "\n",
            "Word: tech\n",
            "Porter Stemmer: tech\n",
            "Lancaster Stemmer: tech\n",
            "Lemmatizer: tech\n",
            "\n",
            "Word: trends\n",
            "Porter Stemmer: trend\n",
            "Lancaster Stemmer: trend\n",
            "Lemmatizer: trend\n",
            "\n",
            "Word: understanding\n",
            "Porter Stemmer: understand\n",
            "Lancaster Stemmer: understand\n",
            "Lemmatizer: understanding\n",
            "\n",
            "Word: impact\n",
            "Porter Stemmer: impact\n",
            "Lancaster Stemmer: impact\n",
            "Lemmatizer: impact\n",
            "\n",
            "Word: society\n",
            "Porter Stemmer: societi\n",
            "Lancaster Stemmer: socy\n",
            "Lemmatizer: society\n",
            "\n",
            "Word: amazing\n",
            "Porter Stemmer: amaz\n",
            "Lancaster Stemmer: amaz\n",
            "Lemmatizer: amazing\n",
            "\n",
            "Word: quickly\n",
            "Porter Stemmer: quickli\n",
            "Lancaster Stemmer: quick\n",
            "Lemmatizer: quickly\n",
            "\n",
            "Word: software\n",
            "Porter Stemmer: softwar\n",
            "Lancaster Stemmer: softw\n",
            "Lemmatizer: software\n",
            "\n",
            "Word: hardware\n",
            "Porter Stemmer: hardwar\n",
            "Lancaster Stemmer: hardw\n",
            "Lemmatizer: hardware\n",
            "\n",
            "Word: changing\n",
            "Porter Stemmer: chang\n",
            "Lancaster Stemmer: chang\n",
            "Lemmatizer: changing\n",
            "\n",
            "Word: new\n",
            "Porter Stemmer: new\n",
            "Lancaster Stemmer: new\n",
            "Lemmatizer: new\n",
            "\n",
            "Word: advancement\n",
            "Porter Stemmer: advanc\n",
            "Lancaster Stemmer: adv\n",
            "Lemmatizer: advancement\n",
            "\n",
            "Word: future\n",
            "Porter Stemmer: futur\n",
            "Lancaster Stemmer: fut\n",
            "Lemmatizer: future\n",
            "\n",
            "Word: becomes\n",
            "Porter Stemmer: becom\n",
            "Lancaster Stemmer: becom\n",
            "Lemmatizer: becomes\n",
            "\n",
            "Word: exciting\n",
            "Porter Stemmer: excit\n",
            "Lancaster Stemmer: excit\n",
            "Lemmatizer: exciting\n",
            "\n",
            "Word: unpredictable\n",
            "Porter Stemmer: unpredict\n",
            "Lancaster Stemmer: unpredict\n",
            "Lemmatizer: unpredictable\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"Stemming and Lemmatization Results:\")\n",
        "for word in filtered_words:\n",
        "    print(\"Word:\", word)\n",
        "    print(\"Porter Stemmer:\", porter.stem(word))\n",
        "    print(\"Lancaster Stemmer:\", lancaster.stem(word))\n",
        "    print(\"Lemmatizer:\", lemmatizer.lemmatize(word))\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAhH0b9UBml_"
      },
      "source": [
        "**Q3. Regular Expressions and Text Spliƫng\n",
        "1.Take their original text from QuesƟon 1.\n",
        "2.Use regular expressions to:\n",
        "a. Extract all words with more than 5 leƩers.\n",
        "b. Extract all numbers (if any exist in their text).\n",
        "c. Extract all capitalized words.\n",
        "3.Use text spliƫng techniques to:\n",
        "a. Split the text into words containing only alphabets (removing digits and special\n",
        "characters).\n",
        "b. Extract words starƟng with a vowel.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBwnyl5e_Zws",
        "outputId": "a0292554-8ae6-4104-b1d3-5c46ee8b047e"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "text = \"Alice scored 95 in English, 89 in Math, and 100 in Science! She loves studying Algorithms and Data Structures.\"\n",
        "\n",
        "words_gt5 = re.findall(r'\\b[a-zA-Z]{6,}\\b', text)\n",
        "\n",
        "numbers = re.findall(r'\\b\\d+\\b', text)\n",
        "\n",
        "capitalized_words = re.findall(r'\\b[A-Z][a-z]*\\b', text)\n",
        "\n",
        "alpha_words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
        "\n",
        "vowel_words = [word for word in alpha_words if re.match(r'^[aeiouAEIOU]', word)]\n",
        "\n",
        "print(\"Words with more than 5 letters:\", words_gt5)\n",
        "print(\"Numbers in the text:\", numbers)\n",
        "print(\"Capitalized words:\", capitalized_words)\n",
        "print(\"Words with only alphabets:\", alpha_words)\n",
        "print(\"Words starting with a vowel:\", vowel_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06q1B1EIBzAm"
      },
      "source": [
        "**Q4. Custom TokenizaƟon & Regex-based Text Cleaning\n",
        "1.Take original text from QuesƟon 1.\n",
        "2.Write a custom tokenizaƟon funcƟon that:\n",
        "a. Removes punctuaƟon and special symbols, but keeps contracƟons (e.g.,\n",
        "\"isn't\" should not be split into \"is\" and \"n't\").\n",
        "b. Handles hyphenated words as a single token (e.g., \"state-of-the-art\" remains\n",
        "a single token).\n",
        "c. Tokenizes numbers separately but keeps decimal numbers intact (e.g., \"3.14\"\n",
        "should remain as is).\n",
        "3.Use Regex SubsƟtuƟons (re.sub) to:\n",
        "a. Replace email addresses with '<EMAIL>' placeholder.\n",
        "b. Replace URLs with '<URL>' placeholder.\n",
        "c. Replace phone numbers (formats: 123-456-7890 or +91 9876543210) with\n",
        "'<PHONE>' placeholder.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ol3AA0VH_xEj",
        "outputId": "66fba164-5bc5-4836-9dfa-fe2a4243c728"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After substitutions: This is Dr. Smith's email: <EMAIL>. More details at <URL> The price is 12.50 dollars. Call <PHONE> or +91 9876543210 for assistance. Please note: don't panic at the self-driving car! \n",
            "\n",
            "Custom tokens: ['This', 'is', 'Dr', \"Smith's\", 'email', 'EMAIL', 'More', 'details', 'at', 'URL', 'The', 'price', 'is', '12.50', 'dollars', 'Call', 'PHONE', 'or', '91', '9876543210', 'for', 'assistance', 'Please', 'note', \"don't\", 'panic', 'at', 'the', 'self-driving', 'car']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "text_new = (\n",
        "    \"This is Dr. Smith's email: example@mail.com. More details at https://www.example.com/info. \"\n",
        "    \"The price is 12.50 dollars. Call 123-456-7890 or +91 9876543210 for assistance. \"\n",
        "    \"Please note: don't panic at the self-driving car!\"\n",
        ")\n",
        "\n",
        "text_clean = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '<EMAIL>', text_new)\n",
        "text_clean = re.sub(r'https?://\\S+|www\\.\\S+', '<URL>', text_clean)\n",
        "text_clean = re.sub(r'\\b(\\d{3}-\\d{3}-\\d{4}|\\+\\d{1,3}\\s*\\d{10})\\b', '<PHONE>', text_clean)\n",
        "print(\"After substitutions:\", text_clean, \"\\n\")\n",
        "\n",
        "pattern = r\"\\d+(?:\\.\\d+)?|[A-Za-z]+(?:[-'][A-Za-z]+)*\"\n",
        "tokenizer = RegexpTokenizer(pattern)\n",
        "custom_tokens = tokenizer.tokenize(text_clean)\n",
        "print(\"Custom tokens:\", custom_tokens)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
